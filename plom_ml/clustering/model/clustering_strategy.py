# SPDX-License-Identifier: AGPL-3.0-or-later
# Copyright (C) 2025 Bryan Tanady

# sklearn
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering


# plom_ml
from plom_ml.clustering.embedding.embedder import (
    Embedder,
    SymbolicEmbedder,
    TrOCREmbedder,
    MCQEmbedder,
)
from plom_ml.clustering.exceptions import MissingEmbedderException

# misc
from abc import abstractmethod
import numpy as np
import yaml
import os
from typing import Mapping


def get_best_clustering(
    X: np.ndarray, thresholds: np.ndarray, metric="silhouette"
) -> np.ndarray:
    """Get the best clustering of X by searching for optimal threshold that maximizes the metric.

    This function defaults with AgglomerativeClustering clustering algorithm. Note that to get
    more fine-grained clustering one can provide smaller thresholds range. Furthermore, it would
    be even better if we provide another argument where we force the threshold at a specific value
    and if that val is None then we do the search.

    Args:
        X: the feature matrix.
        thresholds: the choices of distance thresholds.
        metric: which metric to optimize. Currently supports: "silhouette" and "davies".

    Returns:
        A numpy array of clusterId where the order matches with the
        inputs (index 0 provides Id for row 0 of X)
    """
    best_score = -np.inf if metric == "silhouette" else np.inf
    best_labels = np.array([])

    for t in thresholds:
        clustering = AgglomerativeClustering(
            n_clusters=None, metric="euclidean", distance_threshold=t
        )
        labels = clustering.fit_predict(X)
        # need at least 2 clusters to score
        if len(set(labels)) < 2:
            continue

        if metric == "silhouette":
            score = silhouette_score(X, labels)
            # silhouette: higher -> better
            if score > best_score:
                best_score = score
                best_labels = labels

        elif metric == "davies":
            score = davies_bouldin_score(X, labels)
            # DB index: lower -> better
            if score < best_score:
                best_score = score
                best_labels = labels

    return best_labels


class ClusteringStrategy:
    """Abstract base class for clustering strategy.

    A clustering strategy defines:
        a. Embedders used to generate features for PREPROCESSED inputs
        b. clustering algorithm used to cluster the generated features.
        c. Any task-specific logic eg: PCA decomposition before clustering

    This class enforces every subclass to implement cluster_papers that outputs the
    paper_number to their clusterId. Furthermore the class has get_embeddings method
    that generate a feature vector for an image. The feature vector is a concatenation of
    embeddings generated by the embedderes.
    """

    embedders: list[Embedder]

    def get_embeddings(self, image: np.ndarray) -> np.ndarray:
        """Generate an array of embeddings generated by embedders for the inputted image.

        Args:
            image: the image whose feature vector will be generated for clustering.

        Returns:
            a 1D array of feature vector with shape of shape (D,). D is the sum of output
            dimensions from all embedders.

        Raises:
            MissingEmbedderException: if ClusteringStrategy has not initialized embedders property.
        """
        if not hasattr(self, "embedders") or not self.embedders:
            raise MissingEmbedderException(
                f"Missing self.embedders in {self.__class__.__name__} ClusteringStrategy"
            )

        return np.concatenate([embedder.embed(image) for embedder in self.embedders])

    @abstractmethod
    def cluster_papers(
        self, paper_to_image: Mapping[int, np.ndarray]
    ) -> dict[int, int]:
        """Cluster the given papers into a mapping of paper_num to clusterId.

        This method directly calls inference models on the provided images. Therefore, if
        there are expected preprocessing steps, the images must be preprocessed before
        feeding them into this function.

        Args:
            paper_to_image: a dictionary mapping paper number to a (processed) image that
            represents that paper.

        Returns:
            A dictionary mapping the paper number to their cluster id.
        """
        pass


class HMEClusteringStrategy(ClusteringStrategy):
    """Handwritten math expression model."""

    def __init__(self):
        # load model config
        config_path = os.path.join(os.path.dirname(__file__), "model_config.yaml")
        with open(config_path) as f:
            config = yaml.safe_load(f)

        # load weights path
        symbolic_model_path = config["models"]["hme"]["symbolic"]
        trocr_model_path = config["models"]["hme"]["trocr"]

        # Init embedders
        self.embedders = [
            SymbolicEmbedder(symbolic_model_path),
            TrOCREmbedder(trocr_model_path),
        ]

    def cluster_papers(
        self, paper_to_image: Mapping[int, np.ndarray]
    ) -> dict[int, int]:
        """Cluster the given papers.

        Args:
            paper_to_image: a dictionary mapping paper number to a (processed) image.

        Returns:
            A dictionary mapping the paper number to their cluster id.
        """
        # Build feature matrix
        X = np.vstack(
            [self.get_embeddings(image) for pn, image in paper_to_image.items()]
        )

        X_reduced = PCA(n_components=min(len(paper_to_image), 50)).fit_transform(X)

        # set up distance threshold search space
        min_thresh = 4
        max_thresh = 10
        thresh_counts = 100
        thresholds = np.linspace(min_thresh, max_thresh, thresh_counts)

        clusterIDs = get_best_clustering(X_reduced, thresholds, "davies")
        return dict(zip(list(paper_to_image.keys()), clusterIDs))


class MCQClusteringStrategy(ClusteringStrategy):
    """Handwritten MCQ clustering model."""

    def __init__(self):
        # load model config
        config_path = os.path.join(os.path.dirname(__file__), "model_config.yaml")
        with open(config_path) as f:
            config = yaml.safe_load(f)

        # load weight path
        weight_path = config["models"]["mcq"]

        # init embedder
        out_features = 11
        self.embedders = [
            MCQEmbedder(weight_path=weight_path, out_features=out_features)
        ]

    def cluster_papers(
        self, paper_to_image: Mapping[int, np.ndarray]
    ) -> dict[int, int]:
        """Cluster papers based on handwritten MCQ.

        Args:
            paper_to_image: a dictionary mapping paper number to the
                cropped region used for clustering.

        Returns:
            A dictionary mapping the paper number to their cluster id
        """
        # Build feature matrix
        X = np.vstack(
            [self.get_embeddings(image) for _, image in paper_to_image.items()]
        )

        # NOTE: this threshold space is empirically tuned with custom dataset
        # to enforce more fine-grained cluster move the threshold to smaller value range.
        thresholds = np.linspace(0.05, 0.3, 50)
        clusterIDs = get_best_clustering(X, thresholds=thresholds, metric="silhouette")
        return dict(zip(list(paper_to_image.keys()), clusterIDs))
